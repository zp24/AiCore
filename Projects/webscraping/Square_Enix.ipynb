{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webscraper import Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper() #call scraper class\n",
    "scraper.accept_cookies()\n",
    "#scraper.navigate()\n",
    "scraper.search_bar(\"final fantasy\") #add search keyword here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager #installs Chrome webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "\n",
    "container = scraper.find_container()\n",
    "'''\n",
    "#keep scrolling down page until no more results load up - will run properly if there is no other method being called in same cell\n",
    "SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "# Get scroll height\n",
    "last_height = scraper.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    scraper.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = scraper.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_links = scraper.collect_page_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import uuid #universally unique id\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os, shutil, os.path\n",
    "\n",
    "#add link and product info to dictionary\n",
    "product_dict = {\"product_link\": [], \"product_name\" :[], \"price\" : [], \"description\" : [], \"product_id\": [], \"product_uuid\":[]}\n",
    "image_dict = {\"image_link\": [], \"product_uuid\": [], \"image_uuid\": [], \"image\": []}\n",
    "\n",
    "def retrieve_data():\n",
    "   #link_list #lists all urls in specified webpage section\n",
    "   \n",
    "   for link in product_links[:50]: #iterate through first 50 links\n",
    "      '''\n",
    "         passes any links which relate to games as DOB is required \n",
    "         otherwise any images downloaded from these pages are part of the DOB frame\n",
    "         --> 50 data entries will not be scraped as a result\n",
    "      '''\n",
    "      if \"download\" in link:\n",
    "         pass\n",
    "      elif \"remastered\" in link:\n",
    "         pass\n",
    "      elif \"pc\" in link:\n",
    "         pass\n",
    "      elif \"ps4\" in link:\n",
    "         pass\n",
    "      elif \"switch\" in link:\n",
    "         pass\n",
    "      elif \"xbox\" in link:\n",
    "         pass\n",
    "      elif \"dlc\" in link:\n",
    "         pass\n",
    "      elif \"xiii\" in link:\n",
    "         pass\n",
    "      elif \"xii\" in link:\n",
    "         pass\n",
    "      elif \"starter\" in link:\n",
    "         pass\n",
    "      elif \"ps5\" in link:\n",
    "         pass\n",
    "      elif \"mac\" in link:\n",
    "         pass\n",
    "      elif \"vita\" in link:\n",
    "         pass\n",
    "      else:\n",
    "         scraper.driver.get(link)\n",
    "         time.sleep(2)\n",
    "         product_dict[\"product_link\"].append(link)\n",
    "\n",
    "         try: #get product name\n",
    "            product_name = scraper.driver.find_element(By.XPATH, \n",
    "               '//*[@id=\"desktop-wrapper-title\"]/div/header/h1')\n",
    "               #shortened form of //*[@id=\"responsive-wrapper-title\"]/header/h1\n",
    "               #.//h1[@class=\"product-title\"] - works some of the time\n",
    "               #//*[@id=\"desktop-wrapper-title\"]/div/header/h1\n",
    "            product_dict[\"product_name\"].append(product_name.text)\n",
    "            print(\"Product name obtained\")\n",
    "         except NoSuchElementException: #not all links accessed will have the same attributes\n",
    "            product_dict[\"product_name\"].append(\"N/A\")\n",
    "         \n",
    "         time.sleep(3)\n",
    "         try: #get price\n",
    "            price = scraper.driver.find_element(By.XPATH, '//span[@class = \"prices\"]')\n",
    "            #//*[@id=\"main-content\"]/article/div[1]/div/div[2]/div[2]/div[1]/div[1]/div/span/span\n",
    "               #shortened form of //*[@id=\"main-content\"]/article/div[1]/div/div[2]/div[2]/div[1]/div[1]/div/span\n",
    "            product_dict[\"price\"].append(price.text)\n",
    "            print(\"Price obtained\")\n",
    "         except NoSuchElementException:\n",
    "            product_dict[\"price\"].append(\"N/A\")\n",
    "         \n",
    "         time.sleep(3)\n",
    "         try: #get product description\n",
    "            desc = scraper.driver.find_element(By.XPATH, './/div[@class=\"tab-pane-content\"]')\n",
    "               #shortened form of //*[@id=\"desc-collapse\"]/div/div\n",
    "            product_dict[\"description\"].append(desc.text)\n",
    "            print(\"Product description obtained\")\n",
    "         except NoSuchElementException:\n",
    "            product_dict[\"description\"].append(\"N/A\")\n",
    "         \n",
    "         time.sleep(3)\n",
    "         try: #get product SKU/ID from URL\n",
    "            #to get SKU - product details tab needs to be clicked and then an if statement to get the SKU otherwise other info will be obtained\n",
    "            r = link.rsplit(\"/\", 6) #split link 6 times after every '/'\n",
    "            product_dict[\"product_id\"].append(r[5])\n",
    "            print(\"Product ID obtained\")\n",
    "         except NoSuchElementException:\n",
    "            product_dict[\"product_id\"].append(\"N/A\")\n",
    "\n",
    "         time.sleep(3)\n",
    "         try: #generate V4 UUID for product and image\n",
    "            product_uuid = uuid.uuid4()\n",
    "            image_uuid = uuid.uuid4()\n",
    "            product_dict[\"product_uuid\"].append(product_uuid), image_dict[\"product_uuid\"].append(product_uuid)\n",
    "            image_dict[\"image_uuid\"].append(image_uuid)\n",
    "\n",
    "            print(\"UUID generated\")\n",
    "         except NoSuchElementException:\n",
    "            product_dict[\"product_uuid\"].append(\"N/A\")\n",
    "            image_dict[\"product_uuid\"].append(\"N/A\")\n",
    "            image_dict[\"image_uuid\"].append(\"N/A\")\n",
    "         \n",
    "         time.sleep(3)\n",
    "        \n",
    "         try: #download and save product image using product ID as file name\n",
    "            '''\n",
    "               ensure window is maximised for images to be downloaded successfully (method already implemented)\n",
    "            '''\n",
    "            image = f\"{r[5]}.png\"\n",
    "            with open(image, \"wb\") as file: #wb = write and binary mode\n",
    "               img = scraper.driver.find_element(By.XPATH, \n",
    "                  '//img[@class=\"boxshot lazyloaded\"]')\n",
    "               #//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[2]/figure/img\n",
    "               file.write(img.screenshot_as_png)\n",
    "               time.sleep(3)\n",
    "            image_dict[\"image\"].append(image)\n",
    "            print(\"Image downloaded\")\n",
    "         except:\n",
    "            image_dict[\"image\"].append(\"N/A\")\n",
    "            print(\"Image not downloaded\")\n",
    "         time.sleep(3)\n",
    "\n",
    "         try:\n",
    "            html_page = requests.get(link).content #send get request for each link in range and get content of each request\n",
    "            soup = BeautifulSoup(html_page)\n",
    "            images = []\n",
    "            for img in soup.find_all(\"img\"): #finds .jpg images only\n",
    "               images.append(img.get(\"data-lazy\")) #try \"srcset\" or \"data-srcset\" if this doesn't work\n",
    "                \n",
    "            #add image link(s) to image_dict\n",
    "            image_link = images[2] #retrieves image link only\n",
    "            image_dict[\"image_link\"].append(image_link)\n",
    "            print(\"Image link added\")\n",
    "         except:\n",
    "            image_dict[\"image_link\"].append(\"N/A\")\n",
    "            print(\"Image link not added\")\n",
    "         time.sleep(3)\n",
    "      \n",
    "retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_products = pd.DataFrame(product_dict) #displays product dictionary in panda dataframe\n",
    "df_images = pd.DataFrame(image_dict) #displays image dictionary in panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_images\n",
    "df_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "from uuid import UUID\n",
    "\n",
    "create_data_folder = \"../webscraping/raw_data\"\n",
    "product_file = \"data\"\n",
    "image_file = \"image_data\"\n",
    "create_file = os.path.join(create_data_folder, product_file+\".json\") #add file type here\n",
    "create_image_file = os.path.join(create_data_folder, image_file+\".json\")\n",
    "\n",
    "#Dealing with no UUID serialization support in json\n",
    "JSONEncoder_olddefault = JSONEncoder.default\n",
    "def JSONEncoder_newdefault(self, o):\n",
    "    if isinstance(o, UUID): return str(o)\n",
    "    return JSONEncoder_olddefault(self, o)\n",
    "JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "\n",
    "try: #create raw_data folder in current directory - check if folder already exists\n",
    "    if not os.path.exists(create_data_folder): #if folder doesn't exist\n",
    "        os.mkdir(create_data_folder) \n",
    "        with open(create_file, \"w\") as fp: #specify path here - create data.json file\n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "        with open(create_image_file, \"w\") as fpi:  #create image_data.json file\n",
    "                json.dump(image_dict, fpi,  indent=4)\n",
    "\n",
    "    elif os.path.exists(create_data_folder): #if folder already exists\n",
    "        with open(create_file, \"w\") as fp: \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "        with open(create_image_file, \"w\") as fpi: \n",
    "                json.dump(image_dict, fpi,  indent=4)\n",
    "except FileExistsError:\n",
    "    print(\"Already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move files into a new directory/folder\n",
    "import os, shutil, os.path #shutil - to save it locally\n",
    "\n",
    "create_image_folder = \"images\" #create images folder in current directory\n",
    "sourcepath = \"../webscraping\"\n",
    "source = os.listdir(sourcepath)\n",
    "destinationpath = \"../webscraping/images\"\n",
    "\n",
    "try:\n",
    " if not os.path.exists(create_image_folder):\n",
    "  os.mkdir(create_image_folder)\n",
    "  for files in source:\n",
    "      if files.endswith('.png'):\n",
    "          shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files))  \n",
    " elif os.path.exists(create_image_folder):\n",
    "      for files in source:\n",
    "        if files.endswith('.png'):\n",
    "            shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files)) \n",
    "except FileExistsError:\n",
    " print(\"Already exists\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96845aca18e994caba59710a135a9f2069490d583ecd78ead3231a9fecf57406"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

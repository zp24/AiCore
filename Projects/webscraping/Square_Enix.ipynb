{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webscraper import Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Could not get version for google-chrome with the command:  powershell \"$ErrorActionPreference='silentlycontinue' ; (Get-Item -Path \"$env:PROGRAMFILES\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion ; if (-not $? -or $? -match $error) { (Get-Item -Path \"$env:PROGRAMFILES(x86)\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion } if (-not $? -or $? -match $error) { (Get-Item -Path \"$env:LOCALAPPDATA\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion } if (-not $? -or $? -match $error) { reg query \"HKCU\\SOFTWARE\\Google\\Chrome\\BLBeacon\" /v version } if (-not $? -or $? -match $error) { reg query \"HKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\Google Chrome\" /v version }\"\n",
      "Current google-chrome version is UNKNOWN\n",
      "Get LATEST chromedriver version for UNKNOWN google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/99.0.4844.51/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\Zoya\\.wdm\\drivers\\chromedriver\\win32\\99.0.4844.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage loaded successfully\n",
      "'Accept Cookies' button clicked\n",
      "Search bar opened\n",
      "Search keywords entered\n",
      "Submit search button clicked - redirected to results\n"
     ]
    }
   ],
   "source": [
    "scraper = Scraper() #call scraper class\n",
    "scraper.accept_cookies()\n",
    "scraper.navigate()\n",
    "scraper.search_bar(\"final fantasy\") #add search keyword here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager #installs Chrome webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "container = scraper.find_container()\n",
    "#find many elements that correspond with the XPath - they have to be direct children of the container\n",
    "#i.e. one level below the container\n",
    "list_products = container.find_elements(By.XPATH, './/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "for product in list_products: #iterate through each product\n",
    "    #print(product.text) #print each product in text format\n",
    "    link_list.append(product.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image downloaded\n",
      "Image link added\n",
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image downloaded\n",
      "Image link added\n",
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image downloaded\n",
      "Image link added\n",
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image downloaded\n",
      "Image link added\n",
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image downloaded\n",
      "Image link added\n"
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import uuid #universally unique id\n",
    "import time\n",
    "\n",
    "#add link and product info to dictionary\n",
    "product_dict = {\"Link\": [], \"Product Name\" :[], \"Price\" : [], \"Description\" : [], \"ID\": [], \"UUID\":[]}\n",
    "image_dict = {\"Link\": [], \"Product UUID\": [], \"Image UUID\": []}\n",
    "\n",
    "def retrieve_data():\n",
    "   #link_list #lists all urls in specified webpage section\n",
    "   for link in link_list[3:8]: #iterates through links 4 to 9 - FFVIII downloads not included (DOB required)\n",
    "      scraper.driver.get(link)\n",
    "      time.sleep(2)\n",
    "      product_dict[\"Link\"].append(link)\n",
    "      try: #get product name\n",
    "         product_name = scraper.driver.find_element(By.XPATH, \n",
    "            '//*[@id=\"responsive-wrapper-title\"]/header/h1')\n",
    "            #shortened form of //*[@id=\"responsive-wrapper-title\"]/header/h1\n",
    "            #.//h1[@class=\"product-title\"] - works some of the time\n",
    "         product_dict[\"Product Name\"].append(product_name.text)\n",
    "         print(\"Product name obtained\")\n",
    "      except NoSuchElementException: #not all links accessed will have the same attributes\n",
    "         product_dict[\"Product Name\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get price\n",
    "         price = scraper.driver.find_element(By.XPATH, './/span[@class = \"prices\"]')\n",
    "            #shortened form of //*[@id=\"main-content\"]/article/div[1]/div/div[2]/div[2]/div[1]/div[1]/div/span\n",
    "         product_dict[\"Price\"].append(price.text)\n",
    "         print(\"Price obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict[\"Price\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get product description\n",
    "         desc = scraper.driver.find_element(By.XPATH, './/div[@class=\"tab-pane-content\"]')\n",
    "            #shortened form of //*[@id=\"desc-collapse\"]/div/div\n",
    "         product_dict[\"Description\"].append(desc.text)\n",
    "         print(\"Product description obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict[\"Description\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get product SKU/ID from URL\n",
    "         #to get SKU - product details tab needs to be clicked and then an if statement to get the SKU otherwise other info will be obtained\n",
    "         r = link.rsplit(\"/\", 6) #split link 6 times after every '/'\n",
    "         product_dict[\"ID\"].append(r[5])\n",
    "\n",
    "         print(\"Product ID obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict[\"ID\"].append(\"N/A\")\n",
    "\n",
    "      time.sleep(3)\n",
    "      try: #generate V4 UUID for product and image\n",
    "         product_uuid = uuid.uuid4()\n",
    "         image_uuid = uuid.uuid4()\n",
    "         product_dict[\"UUID\"].append(product_uuid), image_dict[\"Product UUID\"].append(product_uuid)\n",
    "         image_dict[\"Image UUID\"].append(image_uuid)\n",
    "\n",
    "         print(\"UUID generated\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict[\"UUID\"].append(\"N/A\")\n",
    "         image_dict[\"Product UUID\"].append(\"N/A\")\n",
    "         image_dict[\"Image UUID\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #download and save product image using product ID as file name\n",
    "         image_link = f\"{r[5]}.png\"\n",
    "         with open(image_link, \"wb\") as file: #wb = write and binary mode\n",
    "            img = scraper.driver.find_element(By.XPATH, \n",
    "               '//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img')\n",
    "            file.write(img.screenshot_as_png)\n",
    "            time.sleep(3)\n",
    "         print(\"Product image downloaded\")\n",
    "         image_dict[\"Link\"].append(image_link)\n",
    "         print(\"Image link added\")\n",
    "      except:\n",
    "         image_dict[\"Link\"].append(\"N/A\")\n",
    "         print(\"Product image not downloaded\")\n",
    "      time.sleep(3)\n",
    "retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>ID</th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td>FINAL FANTASY MINI PLUSH: FINAL FANTASY IX ZIDANE</td>\n",
       "      <td>£15.99</td>\n",
       "      <td>Zidane of FINAL FANTASY IX is now available as...</td>\n",
       "      <td>462642</td>\n",
       "      <td>1ace2f12-04c5-4a01-8232-ee2befc67057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td>FINAL FANTASY JIGSAW PUZZLE - FINAL FANTASY VI...</td>\n",
       "      <td>£14.99</td>\n",
       "      <td>Square Enix is proud to offer this 500-piece j...</td>\n",
       "      <td>604828</td>\n",
       "      <td>1b2e0d26-79ff-40d5-963d-29da243aee11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td>FINAL FANTASY VII</td>\n",
       "      <td>from £9.99 £4.99</td>\n",
       "      <td>Steam key issued upon purchase. Activate via y...</td>\n",
       "      <td>603952</td>\n",
       "      <td>b8b4790f-0d7f-4316-804a-4f326ea95be9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td>FINAL FANTASY MAT - CHOCOBO</td>\n",
       "      <td>£23.99</td>\n",
       "      <td>Amazingly cute mats of Chocobo, Cactuar, and M...</td>\n",
       "      <td>598069</td>\n",
       "      <td>dd268b7b-0078-49b7-bc35-431cca88766a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td>FINAL FANTASY PLUSH [CACTUAR]</td>\n",
       "      <td>£22.99</td>\n",
       "      <td>A plush toy of Cactuar and one of the traditio...</td>\n",
       "      <td>306733</td>\n",
       "      <td>15e0f2f9-cbbb-463e-aa2a-ecdaa060d4e2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://store.eu.square-enix-games.com/en_GB/p...   \n",
       "1  https://store.eu.square-enix-games.com/en_GB/p...   \n",
       "2  https://store.eu.square-enix-games.com/en_GB/p...   \n",
       "3  https://store.eu.square-enix-games.com/en_GB/p...   \n",
       "4  https://store.eu.square-enix-games.com/en_GB/p...   \n",
       "\n",
       "                                        Product Name             Price  \\\n",
       "0  FINAL FANTASY MINI PLUSH: FINAL FANTASY IX ZIDANE            £15.99   \n",
       "1  FINAL FANTASY JIGSAW PUZZLE - FINAL FANTASY VI...            £14.99   \n",
       "2                                  FINAL FANTASY VII  from £9.99 £4.99   \n",
       "3                        FINAL FANTASY MAT - CHOCOBO            £23.99   \n",
       "4                      FINAL FANTASY PLUSH [CACTUAR]            £22.99   \n",
       "\n",
       "                                         Description      ID  \\\n",
       "0  Zidane of FINAL FANTASY IX is now available as...  462642   \n",
       "1  Square Enix is proud to offer this 500-piece j...  604828   \n",
       "2  Steam key issued upon purchase. Activate via y...  603952   \n",
       "3  Amazingly cute mats of Chocobo, Cactuar, and M...  598069   \n",
       "4  A plush toy of Cactuar and one of the traditio...  306733   \n",
       "\n",
       "                                   UUID  \n",
       "0  1ace2f12-04c5-4a01-8232-ee2befc67057  \n",
       "1  1b2e0d26-79ff-40d5-963d-29da243aee11  \n",
       "2  b8b4790f-0d7f-4316-804a-4f326ea95be9  \n",
       "3  dd268b7b-0078-49b7-bc35-431cca88766a  \n",
       "4  15e0f2f9-cbbb-463e-aa2a-ecdaa060d4e2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(product_dict) #displays product dictionary in panda dataframe\n",
    "#pd.DataFrame(image_dict) #displays image dictionary in panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "from uuid import UUID\n",
    "\n",
    "new_folder = \"C:/Users/Zoya/Desktop/AiCoreScripts/Projects/webscraping/raw_data/\"\n",
    "file = \"data\"\n",
    "create_file = os.path.join(new_folder, file+\".json\") #add file type here\n",
    "\n",
    "#Dealing with no UUID serialization support in json\n",
    "JSONEncoder_olddefault = JSONEncoder.default\n",
    "def JSONEncoder_newdefault(self, o):\n",
    "    if isinstance(o, UUID): return str(o)\n",
    "    return JSONEncoder_olddefault(self, o)\n",
    "JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "\n",
    "try: #create raw_data folder in current directory - check if folder already exists\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.mkdir(new_folder) #create folder if it doesn't exist\n",
    "        with open(create_file, \"w\") as fp: #specify path here \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "                \n",
    "    elif os.path.exists(new_folder): #if folder already exists\n",
    "        with open(create_file, \"w\") as fp: \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "except FileExistsError:\n",
    "    print(\"Already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move files into a new directory/folder\n",
    "import os, shutil, os.path #shutil - to save it locally\n",
    "\n",
    "new_folder1 = \"images\" #create images folder in current directory\n",
    "sourcepath = \"../webscraping\"\n",
    "source = os.listdir(sourcepath)\n",
    "destinationpath = \"../webscraping/images\"\n",
    "\n",
    "try:\n",
    " if not os.path.exists(new_folder1):\n",
    "  os.mkdir(new_folder1)\n",
    "  for files in source:\n",
    "      if files.endswith('.png'):\n",
    "          shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files))    \n",
    " elif os.path.exists(new_folder1):\n",
    "      for files in source:\n",
    "        if files.endswith('.png'):\n",
    "            shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files)) \n",
    "except FileExistsError:\n",
    " print(\"Already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None]\n",
      "[None]\n",
      "[None, None, None, None]\n",
      "[None, None]\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os, shutil, os.path\n",
    "for link in link_list[3:8]:\n",
    "\n",
    "    html_page = requests.get(link).content #send get request for each link in range and get content of each request\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    images = []\n",
    "    for img in soup.find_all(\"picture\"): #finds .jpg images only\n",
    "        \n",
    "        images.append(img.get(\"src\"))\n",
    "\n",
    "    print(images)\n",
    "    #print(img)\n",
    "    #check = r\".jpg\"\n",
    "    #for _ in os.listdir(images):\n",
    "     #if _.endswith(check):\n",
    "      #  print(_)\n",
    "#for img in images:\n",
    " #   if img.contains('.jpg'): \n",
    "  #      print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlretrieve(images, \u001b[39m\"\u001b[39;49m\u001b[39mtest.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\selenium\\lib\\urllib\\request.py:237\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=220'>221</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39murlretrieve\u001b[39m(url, filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, reporthook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=221'>222</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=222'>223</a>\u001b[0m \u001b[39m    Retrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=223'>224</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=234'>235</a>\u001b[0m \u001b[39m    data file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=235'>236</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=236'>237</a>\u001b[0m     url_type, path \u001b[39m=\u001b[39m _splittype(url)\n\u001b[0;32m    <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=238'>239</a>\u001b[0m     \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mclosing(urlopen(url, data)) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/request.py?line=239'>240</a>\u001b[0m         headers \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39minfo()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\selenium\\lib\\urllib\\parse.py:1040\u001b[0m, in \u001b[0;36m_splittype\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/parse.py?line=1036'>1037</a>\u001b[0m \u001b[39mif\u001b[39;00m _typeprog \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/parse.py?line=1037'>1038</a>\u001b[0m     _typeprog \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m([^/:]+):(.*)\u001b[39m\u001b[39m'\u001b[39m, re\u001b[39m.\u001b[39mDOTALL)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/parse.py?line=1039'>1040</a>\u001b[0m match \u001b[39m=\u001b[39m _typeprog\u001b[39m.\u001b[39;49mmatch(url)\n\u001b[0;32m   <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/parse.py?line=1040'>1041</a>\u001b[0m \u001b[39mif\u001b[39;00m match:\n\u001b[0;32m   <a href='file:///c%3A/Users/Zoya/miniconda3/envs/selenium/lib/urllib/parse.py?line=1041'>1042</a>\u001b[0m     scheme, data \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroups()\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(images, \"test.jpg\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96845aca18e994caba59710a135a9f2069490d583ecd78ead3231a9fecf57406"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

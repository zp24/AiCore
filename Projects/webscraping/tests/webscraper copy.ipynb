{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager #installs Chrome webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #HTTP Library\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install webdriver_manager\n",
    "\n",
    "Installs webdriver in cache, so there is no need to install again\n",
    "The webdriver will also detect the latest version of Chrome, but make sure Chrome is updated to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    #load webpage in initialiser\n",
    "    def __init__(self, url: str = \"https://store.eu.square-enix-games.com/en_GB/\"): #default url\n",
    "        self.driver = Chrome(ChromeDriverManager().install()) \n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            #driver = Chrome() #specify location of chromedriver if downloading webdriver\n",
    "            print(\"Webpage loaded successfully\")\n",
    "        except:\n",
    "            print(\"Webpage not loaded - please check\")\n",
    "\n",
    "    #click accept cookies button on webpage\n",
    "    def accept_cookies(self, xpath: str = '//*[@id=\"onetrust-accept-btn-handler\"]'): \n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            time.sleep(5)\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "            print(\"'Accept Cookies' button clicked\")\n",
    "        except TimeoutException: #in case accept button is not found after 10 seconds by driver\n",
    "            print(\"No cookies found\") \n",
    "\n",
    "    #access and type in search bar\n",
    "    def search_bar(self, text, xpath: str = './/a[@id=\"search-button\"]', \n",
    "                    xpath1: str = '//*[@id=\"search-form-wrapper\"]/form/div/input',\n",
    "                    xpath2: str = './/button[@class=\"btn search-button-submit\"]'): \n",
    "        \n",
    "        #click on search bar icon\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            #self.searchbar = self.driver.find_element_by_xpath (\"/html/body/div[2]/header/div/div[1]/div/nav/div/div[2]/ul[1]/li[1]/a/span[1]/i\")\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "            print(\"Search bar opened\")\n",
    "        except TimeoutException:\n",
    "            print(\"Search bar not found\")\n",
    "        \n",
    "        #open search bar\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, xpath1)))\n",
    "            time.sleep(2)\n",
    "            self.driver.find_element(By.XPATH, xpath1).click()\n",
    "        except TimeoutException:\n",
    "            print(\"Search bar not found - input\")\n",
    "        \n",
    "        #input keywords to search\n",
    "        try:\n",
    "            self.search = self.driver.find_element(By.XPATH, xpath1)\n",
    "            self.search.send_keys(text)\n",
    "            print(\"Search keywords entered\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Cannot input keywords\")\n",
    "        \n",
    "        #submit input\n",
    "        try:\n",
    "            self.search = self.driver.find_element(By.XPATH, xpath2).click()\n",
    "            print(\"Submit search button clicked - redirected to results\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Cannot submit search\")\n",
    "        \n",
    "    \n",
    "    def navigate(self): #navigate tabs\n",
    "        self.tab_select = self.driver.find_element_by_xpath(\"/html/body/div[2]/header/div/div[1]/div/nav/div/div[2]/ul[1]/li[3]\")\n",
    "        time.sleep(2)\n",
    "        self.tab_select.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "    \n",
    "    def find_container(self, xpath: str = '//div[@class=\"catalogue row\"]'):\n",
    "        return self.driver.find_element(By.XPATH, xpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Could not get version for google-chrome with the command:  powershell \"$ErrorActionPreference='silentlycontinue' ; (Get-Item -Path \"$env:PROGRAMFILES\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion ; if (-not $? -or $? -match $error) { (Get-Item -Path \"$env:PROGRAMFILES(x86)\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion } if (-not $? -or $? -match $error) { (Get-Item -Path \"$env:LOCALAPPDATA\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion } if (-not $? -or $? -match $error) { reg query \"HKCU\\SOFTWARE\\Google\\Chrome\\BLBeacon\" /v version } if (-not $? -or $? -match $error) { reg query \"HKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\Google Chrome\" /v version }\"\n",
      "Current google-chrome version is UNKNOWN\n",
      "Get LATEST chromedriver version for UNKNOWN google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/99.0.4844.51/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\Zoya\\.wdm\\drivers\\chromedriver\\win32\\99.0.4844.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage loaded successfully\n",
      "'Accept Cookies' button clicked\n",
      "Search bar opened\n",
      "Search keywords entered\n",
      "Submit search button clicked - redirected to results\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": #will only run methods below if script is run directly\n",
    "    scraper = Scraper() #call scraper class\n",
    "    scraper.accept_cookies()\n",
    "    scraper.navigate()\n",
    "    scraper.search_bar(\"final fantasy\") #add search keyword here\n",
    "    \n",
    "    #scraper.driver.execute_script(f\"window.scrollTo(0,document.body.scrollHeight);\") #scroll straight to bottom of page\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager #installs Chrome webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "container = scraper.find_container()\n",
    "#find many elements that correspond with the XPath - they have to be direct children of the container\n",
    "#i.e. one level below the container\n",
    "#list_products = container.find_elements(By.XPATH,'//div[@class=\"catalogue row\"]') # / = 1 level below\n",
    "list_products = container.find_elements(By.XPATH, './/a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINAL FANTASY VII\\nfrom\\n£9.99\\nADD TO CART'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_products[3].text\n",
    "#print(list_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list1 = []\n",
    "for product in list_products: #iterate through each product\n",
    "    #print(product.text) #print each product in text format\n",
    "    #print(product.find_element(By.XPATH, \".//a\"))\n",
    "    link_list1.append(product.get_attribute(\"href\"))\n",
    "    #link_list.append(product.find_element(By.XPATH, \".//a\").get_attribute(\"href\")) #causes an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image not downloaded\n",
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image not downloaded\n",
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image not downloaded\n",
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image not downloaded\n",
      "Product name obtained\n",
      "Price obtained\n",
      "Product description obtained\n",
      "Product ID obtained\n",
      "UUID generated\n",
      "Product image not downloaded\n"
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import uuid #universally unique id\n",
    "\n",
    "#add link and product info to dictionary\n",
    "product_dict1 = {\"Link\": [], \"Product Name\" :[], \"Price\" : [], \"Description\" : [], \"ID\": [], \"UUID\":[]}\n",
    "image_dict1 = {\"Link\": [], \"Product UUID\": [], \"Image UUID\": []}\n",
    "\n",
    "def retrieve_data():\n",
    "   #link_list #lists all urls in specified webpage section\n",
    "   for link in link_list1[9:14]: #iterates through links 4 to 9 - FFVIII downloads not included (DOB required)\n",
    "      scraper.driver.get(link)\n",
    "      time.sleep(2)\n",
    "      product_dict1[\"Link\"].append(link)\n",
    "      try: #get product name\n",
    "         product_name = scraper.driver.find_element(By.XPATH, '//*[@id=\"responsive-wrapper-title\"]/header/h1')\n",
    "            #shortened form of //*[@id=\"responsive-wrapper-title\"]/header/h1\n",
    "            #.//h1[@class=\"product-title\"] - works some of the time\n",
    "         product_dict1[\"Product Name\"].append(product_name.text)\n",
    "         print(\"Product name obtained\")\n",
    "      except NoSuchElementException: #not all links accessed will have the same attributes\n",
    "         product_dict1[\"Product Name\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get price\n",
    "         price = scraper.driver.find_element(By.XPATH, './/span[@class = \"prices\"]')\n",
    "            #shortened form of //*[@id=\"main-content\"]/article/div[1]/div/div[2]/div[2]/div[1]/div[1]/div/span\n",
    "         product_dict1[\"Price\"].append(price.text)\n",
    "         print(\"Price obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict1[\"Price\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get product description\n",
    "         desc = scraper.driver.find_element(By.XPATH, './/div[@class=\"tab-pane-content\"]')\n",
    "            #shortened form of //*[@id=\"desc-collapse\"]/div/div\n",
    "         product_dict1[\"Description\"].append(desc.text)\n",
    "         print(\"Product description obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict1[\"Description\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get product SKU/ID from URL\n",
    "         #to get SKU - product details tab needs to be clicked and then an if statement to get the SKU otherwise other info will be obtained\n",
    "         r = link.rsplit(\"/\", 6) #split link 6 times after every '/'\n",
    "         product_dict1[\"ID\"].append(r[5])\n",
    "\n",
    "         print(\"Product ID obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict1[\"ID\"].append(\"N/A\")\n",
    "\n",
    "      time.sleep(3)\n",
    "\n",
    "      try: #generate V4 UUID for product and image\n",
    "         product_uuid = uuid.uuid4()\n",
    "         image_uuid = uuid.uuid4()\n",
    "         product_dict1[\"UUID\"].append(product_uuid)\n",
    "         image_dict1[\"Product UUID\"].append(product_uuid)\n",
    "         image_dict1[\"Image UUID\"].append(image_uuid)\n",
    "\n",
    "         print(\"UUID generated\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict1[\"UUID\"].append(\"N/A\")\n",
    "         image_dict1[\"Product UUID\"].append(\"N/A\")\n",
    "         image_dict1[\"Image UUID\"].append(\"N/A\")\n",
    "      time.sleep(3)\n",
    "\n",
    "      try: #download and save product image using product ID as file name\n",
    "         image_link = f\"{r[5]}.jpeg\"\n",
    "         with open(image_link, \"wb\") as file: #wb = write and binary mode\n",
    "            img = scraper.driver.find_element(By.XPATH, './/img[@class\"lazyloaded\"]')\n",
    "            #//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img\n",
    "            #//*[@id=\"main-content\"]/article/div[2]/div/div/div/div[1]/div/a/picture/img\n",
    "            #//*[@id=\"main-content\"]/article/div[2]/div/div/div/div[2]/div/a/picture/img\n",
    "            #//*[@id=\"main-content\"]/article/div[2]/div/div/div/div[3]/div/a/picture/img\n",
    "            #for image in img:\n",
    "             #  x = 0\n",
    "              # file.write(f\"{x}\"+img.screenshot_as_png)\n",
    "               #x+=1\n",
    "            file.write(img.screenshot_as_jpeg)\n",
    "         print(\"Product image downloaded\")\n",
    "         image_dict1[\"Link\"].append(image_link)\n",
    "         print(\"Image link added\")\n",
    "      except:\n",
    "         image_dict1[\"Link\"].append(\"N/A\")\n",
    "         print(\"Product image not downloaded\")\n",
    "      time.sleep(3)\n",
    "retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Link': ['https://store.eu.square-enix-games.com/en_GB/product/603952/final-fantasy-vii',\n",
       "  'https://store.eu.square-enix-games.com/en_GB/product/462642/final-fantasy-mini-plush-final-fantasy-ix-zidane',\n",
       "  'https://store.eu.square-enix-games.com/en_GB/product/598598/final-fantasy-jumbo-cactuar',\n",
       "  'https://store.eu.square-enix-games.com/en_GB/product/598050/final-fantasy-mat-cactuar',\n",
       "  'https://store.eu.square-enix-games.com/en_GB/product/604828/final-fantasy-jigsaw-puzzle-final-fantasy-vii-500-piece'],\n",
       " 'Product Name': ['', '', '', '', ''],\n",
       " 'Price': ['', '', '', '', ''],\n",
       " 'Description': ['Steam key issued upon purchase. Activate via your Steam account\\n\\nIn Midgar, a city controlled by the mega-conglomerate Shinra Inc., the No. 1 Mako Reactor has been blown up by a rebel group, AVALANCHE.\\nAVALANCHE was secretly formed to wage a rebellion against Shinra Inc., an organisation which is absorbing Mako energy, destroying the natural resources of the planet. Cloud, a former member of Shinra´s elite combat force, SOLDIER, was involved with the bombing of the Mako Reactor.\\nCan Cloud and AVALANCHE protect the planet from the huge, formidable enemy, Shinra Inc.?',\n",
       "  'Zidane of FINAL FANTASY IX is now available as a chibi-style mini plush!\\n\\n©SQUARE ENIX CO., LTD. All Rights Reserved.',\n",
       "  'This jumbo plush with be a 1/1 scale as the Cactuar in FINAL FANTASY VII REMAKE.\\nSo get ready for 10,000 needles and a soft hug!\\n\\n* Please note:\\nSmall cactuar in photo is only for size reference. Not included in purchase.\\nProduct Size: Approx. W 475mm × D 160mm × H 680mm',\n",
       "  'Amazingly cute mats of Chocobo, Cactuar, and Moogle are here From the Final Fantasy series! The lovable characters are colorfully recreated and will add a bit of whimsy no matter where in your home they’re used.\\n\\nItem Size:  W 60 cm X H 40 cm\\nMaterial:  Polyester/TPU',\n",
       "  \"Square Enix is proud to offer this 500-piece jigsaw puzzle.\\n\\nFeaturing the classic character illustrations by Tetsuya Nomura from Final Fantasy VII with iconic depictions of Cloud Strife, Tifa Lockhart, Aerith Gainsborough, Barret Wallace, Yuffie Kisaragi, Vincent Valentine, Cait Sith, Cid Highwind and Red XIII in front of Midgar's Mako Generators. Finishing over a square foot, this will be fun for puzzle and Final Fantasy fans of all ages!\\n\\nRelease date is approximate.\\n\\nShipment may take longer if the parcel contains an import product and faces delays due to customs and duties.\\nAlso, please be aware of potential release date changes which shall be updated on the website.\"],\n",
       " 'ID': ['603952', '462642', '598598', '598050', '604828'],\n",
       " 'UUID': [UUID('d839917c-ff40-4fcd-a74e-40a12525c44d'),\n",
       "  UUID('de9e2c33-2a13-4fed-9433-dd4a400ff28f'),\n",
       "  UUID('98dd5da4-c223-4adc-94dc-be7536fbd629'),\n",
       "  UUID('b4cdda25-186b-482f-8ad3-76da80030480'),\n",
       "  UUID('8ba818c4-8679-4591-b65e-431797ddedf3')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dict #display all dictionary entries\n",
    "\n",
    "## print product names and prices ##\n",
    "#print(product_dict[\"Product Name\"], product_dict[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  604828\n"
     ]
    }
   ],
   "source": [
    "test = list(product_dict.values()) #list dictionary values \n",
    "\n",
    "for name in test[1]:\n",
    "        #print(name) #print name\n",
    "        name\n",
    "for price in test[2]:\n",
    "        #print(price)\n",
    "        price\n",
    "for product_id in test[4]:\n",
    "        #print(product_id)\n",
    "        product_id\n",
    "print(name, price, product_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link\n",
      "Product Name\n",
      "Price\n",
      "Description\n",
      "ID\n",
      "UUID\n"
     ]
    }
   ],
   "source": [
    "for keys in product_dict:\n",
    "    print(keys) #prints keys in product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Link': ['N/A', 'N/A', 'N/A', 'N/A', 'N/A'],\n",
       " 'Product UUID': [UUID('d839917c-ff40-4fcd-a74e-40a12525c44d'),\n",
       "  UUID('de9e2c33-2a13-4fed-9433-dd4a400ff28f'),\n",
       "  UUID('98dd5da4-c223-4adc-94dc-be7536fbd629'),\n",
       "  UUID('b4cdda25-186b-482f-8ad3-76da80030480'),\n",
       "  UUID('8ba818c4-8679-4591-b65e-431797ddedf3')],\n",
       " 'Image UUID': [UUID('18a1a512-7c40-489e-b97f-40acf124dbaa'),\n",
       "  UUID('7e28783b-dbcd-4a0c-bc1e-2e8c3d485d78'),\n",
       "  UUID('6df368be-a283-4425-a93c-52415207b84d'),\n",
       "  UUID('4fed2312-0c0f-4214-a457-86a1f95269a5'),\n",
       "  UUID('d9cb14d5-23ab-43e2-90be-55bf84fc8847')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dict #display all dictionary entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>ID</th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Steam key issued upon purchase. Activate via y...</td>\n",
       "      <td>603952</td>\n",
       "      <td>d839917c-ff40-4fcd-a74e-40a12525c44d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Zidane of FINAL FANTASY IX is now available as...</td>\n",
       "      <td>462642</td>\n",
       "      <td>de9e2c33-2a13-4fed-9433-dd4a400ff28f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>This jumbo plush with be a 1/1 scale as the Ca...</td>\n",
       "      <td>598598</td>\n",
       "      <td>98dd5da4-c223-4adc-94dc-be7536fbd629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Amazingly cute mats of Chocobo, Cactuar, and M...</td>\n",
       "      <td>598050</td>\n",
       "      <td>b4cdda25-186b-482f-8ad3-76da80030480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://store.eu.square-enix-games.com/en_GB/p...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Square Enix is proud to offer this 500-piece j...</td>\n",
       "      <td>604828</td>\n",
       "      <td>8ba818c4-8679-4591-b65e-431797ddedf3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link Product Name Price  \\\n",
       "0  https://store.eu.square-enix-games.com/en_GB/p...                      \n",
       "1  https://store.eu.square-enix-games.com/en_GB/p...                      \n",
       "2  https://store.eu.square-enix-games.com/en_GB/p...                      \n",
       "3  https://store.eu.square-enix-games.com/en_GB/p...                      \n",
       "4  https://store.eu.square-enix-games.com/en_GB/p...                      \n",
       "\n",
       "                                         Description      ID  \\\n",
       "0  Steam key issued upon purchase. Activate via y...  603952   \n",
       "1  Zidane of FINAL FANTASY IX is now available as...  462642   \n",
       "2  This jumbo plush with be a 1/1 scale as the Ca...  598598   \n",
       "3  Amazingly cute mats of Chocobo, Cactuar, and M...  598050   \n",
       "4  Square Enix is proud to offer this 500-piece j...  604828   \n",
       "\n",
       "                                   UUID  \n",
       "0  d839917c-ff40-4fcd-a74e-40a12525c44d  \n",
       "1  de9e2c33-2a13-4fed-9433-dd4a400ff28f  \n",
       "2  98dd5da4-c223-4adc-94dc-be7536fbd629  \n",
       "3  b4cdda25-186b-482f-8ad3-76da80030480  \n",
       "4  8ba818c4-8679-4591-b65e-431797ddedf3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(product_dict) #displays product dictionary in panda dataframe\n",
    "#pd.DataFrame(image_dict) #displays image dictionary in panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "from uuid import UUID\n",
    "\n",
    "new_folder = \"C:/Users/Zoya/Desktop/AiCoreScripts/Projects/webscraping/raw_data/\"\n",
    "file = \"data\"\n",
    "create_file = os.path.join(new_folder, file+\".json\") #add file type here\n",
    "\n",
    "#Dealing with no UUID serialization support in json\n",
    "\n",
    "JSONEncoder_olddefault = JSONEncoder.default\n",
    "def JSONEncoder_newdefault(self, o):\n",
    "    if isinstance(o, UUID): return str(o)\n",
    "    return JSONEncoder_olddefault(self, o)\n",
    "JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "#create raw_data folder in current directory \n",
    "try: #check if folder already exists\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.mkdir(new_folder) #create folder if it doesn't exist\n",
    "        with open(create_file, \"w\") as fp: #specify path here \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "                \n",
    "    elif os.path.exists(new_folder): #if folder already exists\n",
    "        with open(create_file, \"w\") as fp: \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "except FileExistsError:\n",
    "    print(\"Already exists\")\n",
    "\n",
    "#f = open(create_file, \"a\") #create new file in specified folder/directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##image.png is overwritten every time##\n",
    "#with open(\"image.png\", \"wb\") as file: #wb = write and binary mode\n",
    " #   img = scraper.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img')\n",
    " #  file.write(img.screenshot_as_png)\n",
    "#scraper.driver.quit() #close window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move files into a new directory/folder\n",
    "import os, shutil\n",
    "import os.path\n",
    "\n",
    "new_folder1 = \"images\" #create images folder in current directory\n",
    "sourcepath = \"C:/Users/Zoya/Desktop/AiCoreScripts/Projects/webscraping\"\n",
    "source = os.listdir(sourcepath)\n",
    "destinationpath = \"C:/Users/Zoya/Desktop/AiCoreScripts/Projects/webscraping/images\"\n",
    "\n",
    "try:\n",
    " if not os.path.exists(new_folder1):\n",
    "  os.mkdir(new_folder1)\n",
    "  for files in source:\n",
    "      if files.endswith('.png'):\n",
    "          shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files))    \n",
    " elif os.path.exists(new_folder1):\n",
    "      for files in source:\n",
    "        if files.endswith('.png'):\n",
    "            shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files)) \n",
    "except FileExistsError:\n",
    " print(\"Already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "\n",
    "#r = requests.get(\"https://store.eu.square-enix-games.com/en_GB/product/598050/final-fantasy-mat-cactuar\")\n",
    "#r #<Response [200]>\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup #translator of html code obtained from requests\n",
    "\n",
    "#soup = BeautifulSoup(r.text, 'html.parser') #makes r.text easier to read in terms of html tags\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find().text #('tag', {id: ''})\n",
    "#soup.find_parent() #replace soup with above variable\n",
    "#soup.find_next_sibling() #replace soup with above variable\n",
    "\n",
    "#soup.text #replace soup with above variable\n",
    "#print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Id : {\n",
    "Id: { ‘product_name’: ‘final fantasy’, ‘description’: ‘game’}\n",
    "ID: ‘1’, {‘product_name’....}\n",
    "So if you’re scraping it and saving it into a data format, you will have to save it as a dictionary with key value pair, the key being the field and the value being the data that is being stored with the key\n",
    "save it as json\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96845aca18e994caba59710a135a9f2069490d583ecd78ead3231a9fecf57406"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

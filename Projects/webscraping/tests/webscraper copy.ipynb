{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager #installs Chrome webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #HTTP Library\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install webdriver_manager\n",
    "\n",
    "Installs webdriver in cache, so there is no need to install again\n",
    "The webdriver will also detect the latest version of Chrome, but make sure Chrome is updated to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    #load webpage in initialiser\n",
    "    def __init__(self, url: str = \"https://store.eu.square-enix-games.com/en_GB/\"): #default url\n",
    "        self.driver = Chrome(ChromeDriverManager().install()) \n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            #driver = Chrome() #specify location of chromedriver if downloading webdriver\n",
    "            print(\"Webpage loaded successfully\")\n",
    "        except:\n",
    "            print(\"Webpage not loaded - please check\")\n",
    "\n",
    "    #click accept cookies button on webpage\n",
    "    def accept_cookies(self, xpath: str = '//*[@id=\"onetrust-accept-btn-handler\"]'): \n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            time.sleep(5)\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "            print(\"'Accept Cookies' button clicked\")\n",
    "        except TimeoutException: #in case accept button is not found after 10 seconds by driver\n",
    "            print(\"No cookies found\") \n",
    "\n",
    "    #access and type in search bar\n",
    "    def search_bar(self, text, xpath: str = './/a[@id=\"search-button\"]', \n",
    "                    xpath1: str = '//*[@id=\"search-form-wrapper\"]/form/div/input',\n",
    "                    xpath2: str = './/button[@class=\"btn search-button-submit\"]'): \n",
    "        \n",
    "        #click on search bar icon\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            #self.searchbar = self.driver.find_element_by_xpath (\"/html/body/div[2]/header/div/div[1]/div/nav/div/div[2]/ul[1]/li[1]/a/span[1]/i\")\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "            print(\"Search bar opened\")\n",
    "        except TimeoutException:\n",
    "            print(\"Search bar not found\")\n",
    "        \n",
    "        #open search bar\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, xpath1)))\n",
    "            time.sleep(2)\n",
    "            self.driver.find_element(By.XPATH, xpath1).click()\n",
    "        except TimeoutException:\n",
    "            print(\"Search bar not found - input\")\n",
    "        \n",
    "        #input keywords to search\n",
    "        try:\n",
    "            self.search = self.driver.find_element(By.XPATH, xpath1)\n",
    "            self.search.send_keys(text)\n",
    "            print(\"Search keywords entered\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Cannot input keywords\")\n",
    "        \n",
    "        #submit input\n",
    "        try:\n",
    "            self.search = self.driver.find_element(By.XPATH, xpath2).click()\n",
    "            print(\"Submit search button clicked - redirected to results\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Cannot submit search\")\n",
    "        \n",
    "    \n",
    "    def navigate(self): #navigate tabs\n",
    "        self.tab_select = self.driver.find_element_by_xpath(\"/html/body/div[2]/header/div/div[1]/div/nav/div/div[2]/ul[1]/li[3]\")\n",
    "        time.sleep(2)\n",
    "        self.tab_select.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "    \n",
    "    def find_container(self, xpath: str = '//div[@class=\"catalogue row\"]'):\n",
    "        return self.driver.find_element(By.XPATH, xpath)\n",
    "    \n",
    "    def collect_page_links(self, xpath : str = \".//a\"):\n",
    "        self.container = scraper.find_container()\n",
    "        self.list_products = self.container.find_elements(By.XPATH, xpath)\n",
    "        self.link_list = []\n",
    "        for product in self.list_products: #iterate through each product\n",
    "            #print(product.text) #print each product in text format\n",
    "            self.link_list.append(product.get_attribute(\"href\"))\n",
    "        \n",
    "        return self.link_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": #will only run methods below if script is run directly\n",
    "    scraper = Scraper() #call scraper class\n",
    "    scraper.accept_cookies()\n",
    "    scraper.navigate()\n",
    "    scraper.search_bar(\"final fantasy\") #add search keyword here\n",
    "    \n",
    "    \n",
    "    #scraper.driver.execute_script(f\"window.scrollTo(0,document.body.scrollHeight);\") #scroll straight to bottom of page\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager #installs Chrome webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "container = scraper.find_container()\n",
    "#find many elements that correspond with the XPath - they have to be direct children of the container\n",
    "#i.e. one level below the container\n",
    "#list_products = container.find_elements(By.XPATH,'//div[@class=\"catalogue row\"]') # / = 1 level below\n",
    "#list_products = container.find_elements(By.XPATH, './/a')\n",
    "#scraper.collect_page_links()\n",
    "container.text[0:18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.list_products[3].text\n",
    "#print(list_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list1 = []\n",
    "for product in scraper.list_products: #iterate through each product\n",
    "    #print(product.text) #print each product in text format\n",
    "    #print(product.find_element(By.XPATH, \".//a\"))\n",
    "    link_list1.append(product.get_attribute(\"href\"))\n",
    "    #link_list.append(product.find_element(By.XPATH, \".//a\").get_attribute(\"href\")) #causes an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import uuid #universally unique id\n",
    "\n",
    "#add link and product info to dictionary\n",
    "product_dict1 = {\"Link\": [], \"Product Name\" :[], \"Price\" : [], \"Description\" : [], \"ID\": [], \"UUID\":[]}\n",
    "image_dict1 = {\"Link\": [], \"Product UUID\": [], \"Image UUID\": []}\n",
    "\n",
    "def retrieve_data():\n",
    "   #link_list #lists all urls in specified webpage section\n",
    "   for link in link_list1[9:14]: #iterates through links 4 to 9 - FFVIII downloads not included (DOB required)\n",
    "      scraper.driver.get(link)\n",
    "      time.sleep(2)\n",
    "      product_dict1[\"Link\"].append(link)\n",
    "      try: #get product name\n",
    "         product_name = scraper.driver.find_element(By.XPATH, '//*[@id=\"responsive-wrapper-title\"]/header/h1')\n",
    "            #shortened form of //*[@id=\"responsive-wrapper-title\"]/header/h1\n",
    "            #.//h1[@class=\"product-title\"] - works some of the time\n",
    "         product_dict1[\"Product Name\"].append(product_name.text)\n",
    "         print(\"Product name obtained\")\n",
    "      except NoSuchElementException: #not all links accessed will have the same attributes\n",
    "         product_dict1[\"Product Name\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get price\n",
    "         price = scraper.driver.find_element(By.XPATH, './/span[@class = \"prices\"]')\n",
    "            #shortened form of //*[@id=\"main-content\"]/article/div[1]/div/div[2]/div[2]/div[1]/div[1]/div/span\n",
    "         product_dict1[\"Price\"].append(price.text)\n",
    "         print(\"Price obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict1[\"Price\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get product description\n",
    "         desc = scraper.driver.find_element(By.XPATH, './/div[@class=\"tab-pane-content\"]')\n",
    "            #shortened form of //*[@id=\"desc-collapse\"]/div/div\n",
    "         product_dict1[\"Description\"].append(desc.text)\n",
    "         print(\"Product description obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict1[\"Description\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get product SKU/ID from URL\n",
    "         #to get SKU - product details tab needs to be clicked and then an if statement to get the SKU otherwise other info will be obtained\n",
    "         r = link.rsplit(\"/\", 6) #split link 6 times after every '/'\n",
    "         product_dict1[\"ID\"].append(r[5])\n",
    "\n",
    "         print(\"Product ID obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict1[\"ID\"].append(\"N/A\")\n",
    "\n",
    "      time.sleep(3)\n",
    "\n",
    "      try: #generate V4 UUID for product and image\n",
    "         product_uuid = uuid.uuid4()\n",
    "         image_uuid = uuid.uuid4()\n",
    "         product_dict1[\"UUID\"].append(product_uuid)\n",
    "         image_dict1[\"Product UUID\"].append(product_uuid)\n",
    "         image_dict1[\"Image UUID\"].append(image_uuid)\n",
    "\n",
    "         print(\"UUID generated\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict1[\"UUID\"].append(\"N/A\")\n",
    "         image_dict1[\"Product UUID\"].append(\"N/A\")\n",
    "         image_dict1[\"Image UUID\"].append(\"N/A\")\n",
    "      time.sleep(3)\n",
    "\n",
    "      try: #download and save product image using product ID as file name\n",
    "         image_link = f\"{r[5]}.jpeg\"\n",
    "         with open(image_link, \"wb\") as file: #wb = write and binary mode\n",
    "            img = scraper.driver.find_element(By.XPATH, \n",
    "               '//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img')\n",
    "            #//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img\n",
    "            #//*[@id=\"main-content\"]/article/div[2]/div/div/div/div[1]/div/a/picture/img\n",
    "            #//*[@id=\"main-content\"]/article/div[2]/div/div/div/div[2]/div/a/picture/img\n",
    "            #//*[@id=\"main-content\"]/article/div[2]/div/div/div/div[3]/div/a/picture/img\n",
    "            #for image in img:\n",
    "             #  x = 0\n",
    "              # file.write(f\"{x}\"+img.screenshot_as_png)\n",
    "               #x+=1\n",
    "            file.write(img.screenshot_as_jpeg)\n",
    "            img1 = scraper.driver.find_element(By.XPATH, \n",
    "               '//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img')\n",
    "         print(\"Product image downloaded\")\n",
    "         image_dict1[\"Link\"].append(image_link)\n",
    "         print(\"Image link added\")\n",
    "      except:\n",
    "         image_dict1[\"Link\"].append(\"N/A\")\n",
    "         print(\"Product image not downloaded\")\n",
    "      time.sleep(3)\n",
    "retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated version\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import uuid #universally unique id\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os, shutil, os.path\n",
    "\n",
    "#add link and product info to dictionary\n",
    "product_dict = {\"Link\": [], \"Product Name\" :[], \"Price\" : [], \"Description\" : [], \"ID\": [], \"UUID\":[]}\n",
    "image_dict = {\"Link\": [], \"Product UUID\": [], \"Image UUID\": [], \"Image\": []}\n",
    "\n",
    "def retrieve_data():\n",
    "   #link_list #lists all urls in specified webpage section\n",
    "   \n",
    "   for link in product_links[:50]:\n",
    "      '''pass any links which relate to games as DOB is required \n",
    "         otherwise any images downloaded from these pages are part of the DOB frame\n",
    "      '''\n",
    "      if \"download\" in link:\n",
    "         pass\n",
    "      elif \"remastered\" in link:\n",
    "         pass\n",
    "      elif \"pc\" in link:\n",
    "         pass\n",
    "      elif \"ps4\" in link:\n",
    "         pass\n",
    "      elif \"switch\" in link:\n",
    "         pass\n",
    "      elif \"xbox\" in link:\n",
    "         pass\n",
    "      elif \"dlc\" in link:\n",
    "         pass\n",
    "      elif \"xiii\" in link:\n",
    "         pass\n",
    "      elif \"starter\" in link:\n",
    "         pass\n",
    "      elif \"ps5\" in link:\n",
    "         pass\n",
    "      elif \"mac\" in link:\n",
    "         pass\n",
    "      else:\n",
    "         scraper.driver.get(link)\n",
    "         time.sleep(2)\n",
    "         product_dict[\"Link\"].append(link)\n",
    "\n",
    "         try: #get product name\n",
    "            product_name = scraper.driver.find_element(By.XPATH, \n",
    "               '//*[@id=\"desktop-wrapper-title\"]/div/header/h1')\n",
    "               #shortened form of //*[@id=\"responsive-wrapper-title\"]/header/h1\n",
    "               #.//h1[@class=\"product-title\"] - works some of the time\n",
    "               #//*[@id=\"desktop-wrapper-title\"]/div/header/h1\n",
    "            product_dict[\"Product Name\"].append(product_name.text)\n",
    "            print(\"Product name obtained\")\n",
    "         except NoSuchElementException: #not all links accessed will have the same attributes\n",
    "            product_dict[\"Product Name\"].append(\"N/A\")\n",
    "         \n",
    "         time.sleep(3)\n",
    "         try: #get price\n",
    "            price = scraper.driver.find_element(By.XPATH, '//span[@class = \"prices\"]')\n",
    "            #//*[@id=\"main-content\"]/article/div[1]/div/div[2]/div[2]/div[1]/div[1]/div/span/span\n",
    "               #shortened form of //*[@id=\"main-content\"]/article/div[1]/div/div[2]/div[2]/div[1]/div[1]/div/span\n",
    "            product_dict[\"Price\"].append(price.text)\n",
    "            print(\"Price obtained\")\n",
    "         except NoSuchElementException:\n",
    "            product_dict[\"Price\"].append(\"N/A\")\n",
    "         \n",
    "         time.sleep(3)\n",
    "         try: #get product description\n",
    "            desc = scraper.driver.find_element(By.XPATH, './/div[@class=\"tab-pane-content\"]')\n",
    "               #shortened form of //*[@id=\"desc-collapse\"]/div/div\n",
    "            product_dict[\"Description\"].append(desc.text)\n",
    "            print(\"Product description obtained\")\n",
    "         except NoSuchElementException:\n",
    "            product_dict[\"Description\"].append(\"N/A\")\n",
    "         \n",
    "         time.sleep(3)\n",
    "         try: #get product SKU/ID from URL\n",
    "            #to get SKU - product details tab needs to be clicked and then an if statement to get the SKU otherwise other info will be obtained\n",
    "            r = link.rsplit(\"/\", 6) #split link 6 times after every '/'\n",
    "            product_dict[\"ID\"].append(r[5])\n",
    "            print(\"Product ID obtained\")\n",
    "         except NoSuchElementException:\n",
    "            product_dict[\"ID\"].append(\"N/A\")\n",
    "\n",
    "         time.sleep(3)\n",
    "         try: #generate V4 UUID for product and image\n",
    "            product_uuid = uuid.uuid4()\n",
    "            image_uuid = uuid.uuid4()\n",
    "            product_dict[\"UUID\"].append(product_uuid), image_dict[\"Product UUID\"].append(product_uuid)\n",
    "            image_dict[\"Image UUID\"].append(image_uuid)\n",
    "\n",
    "            print(\"UUID generated\")\n",
    "         except NoSuchElementException:\n",
    "            product_dict[\"UUID\"].append(\"N/A\")\n",
    "            image_dict[\"Product UUID\"].append(\"N/A\")\n",
    "            image_dict[\"Image UUID\"].append(\"N/A\")\n",
    "         \n",
    "         time.sleep(3)\n",
    "         try: #download and save product image using product ID as file name\n",
    "            image = f\"{r[5]}.png\"\n",
    "            with open(image, \"wb\") as file: #wb = write and binary mode\n",
    "               img = scraper.driver.find_element(By.XPATH, \n",
    "                  '//img[@class=\"boxshot lazyloaded\"]')\n",
    "               #//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[2]/figure/img\n",
    "               file.write(img.screenshot_as_png)\n",
    "               time.sleep(3)\n",
    "            image_dict[\"Image\"].append(image)\n",
    "            print(\"Image downloaded\")\n",
    "         except:\n",
    "            image_dict[\"Image\"].append(\"N/A\")\n",
    "            print(\"Image not downloaded\")\n",
    "         time.sleep(3)\n",
    "\n",
    "         try:\n",
    "            html_page = requests.get(link).content #send get request for each link in range and get content of each request\n",
    "            soup = BeautifulSoup(html_page)\n",
    "            images = []\n",
    "            for img in soup.find_all(\"img\"): #finds .jpg images only\n",
    "               images.append(img.get(\"data-lazy\"))\n",
    "               #s = img.get(\"srcset\")\n",
    "               #st = x[2].split(\"1x\") #split src as srcset contains two links by default \n",
    "                  #src = images.append(img.get(\"srcset\"))\n",
    "                  #s = images.split(\"1x\") #split src as srcset contains two links by default\n",
    "                  #src = scraper.driver.find_element(By.XPATH, './/img').get_attribute('srcset')\n",
    "                  #src_list.append(r[0]) #obtain image from 1st link only\n",
    "            #for image in images:\n",
    "                \n",
    "\n",
    "               #add image link(s) to image_dict\n",
    "            image_link = images[2]\n",
    "            #image_link = s[0]\n",
    "            image_dict[\"Link\"].append(image_link)\n",
    "            print(\"Image link added\")\n",
    "         except:\n",
    "            image_dict[\"Link\"].append(\"N/A\")\n",
    "            print(\"Image link not added\")\n",
    "         time.sleep(3)\n",
    "      \n",
    "retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dict #display all dictionary entries\n",
    "\n",
    "## print product names and prices ##\n",
    "#print(product_dict[\"Product Name\"], product_dict[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(product_dict.values()) #list dictionary values \n",
    "\n",
    "for name in test[1]:\n",
    "        #print(name) #print name\n",
    "        name\n",
    "for price in test[2]:\n",
    "        #print(price)\n",
    "        price\n",
    "for product_id in test[4]:\n",
    "        #print(product_id)\n",
    "        product_id\n",
    "print(name, price, product_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in product_dict:\n",
    "    print(keys) #prints keys in product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict #display all dictionary entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(product_dict) #displays product dictionary in panda dataframe\n",
    "#pd.DataFrame(image_dict) #displays image dictionary in panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "from uuid import UUID\n",
    "\n",
    "new_folder = \"../webscraping/raw_data\"\n",
    "file = \"data\"\n",
    "create_file = os.path.join(new_folder, file+\".json\") #add file type here\n",
    "\n",
    "#Dealing with no UUID serialization support in json\n",
    "\n",
    "JSONEncoder_olddefault = JSONEncoder.default\n",
    "def JSONEncoder_newdefault(self, o):\n",
    "    if isinstance(o, UUID): return str(o)\n",
    "    return JSONEncoder_olddefault(self, o)\n",
    "JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "#create raw_data folder in current directory \n",
    "try: #check if folder already exists\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.mkdir(new_folder) #create folder if it doesn't exist\n",
    "        with open(create_file, \"w\") as fp: #specify path here \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "                \n",
    "    elif os.path.exists(new_folder): #if folder already exists\n",
    "        with open(create_file, \"w\") as fp: \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "except FileExistsError:\n",
    "    print(\"Already exists\")\n",
    "\n",
    "#f = open(create_file, \"a\") #create new file in specified folder/directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##image.png is overwritten every time##\n",
    "#with open(\"image.png\", \"wb\") as file: #wb = write and binary mode\n",
    " #   img = scraper.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img')\n",
    " #  file.write(img.screenshot_as_png)\n",
    "#scraper.driver.quit() #close window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move files into a new directory/folder\n",
    "import os, shutil\n",
    "import os.path\n",
    "\n",
    "new_folder1 = \"images\" #create images folder in current directory\n",
    "sourcepath = \"../webscraping\"\n",
    "source = os.listdir(sourcepath)\n",
    "destinationpath = \"../webscraping/images\"\n",
    "\n",
    "try:\n",
    " if not os.path.exists(new_folder1):\n",
    "  os.mkdir(new_folder1)\n",
    "  for files in source:\n",
    "      if files.endswith('.png'):\n",
    "          shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files))    \n",
    " elif os.path.exists(new_folder1):\n",
    "      for files in source:\n",
    "        if files.endswith('.png'):\n",
    "            shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files)) \n",
    "except FileExistsError:\n",
    " print(\"Already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "\n",
    "#r = requests.get(\"https://store.eu.square-enix-games.com/en_GB/product/598050/final-fantasy-mat-cactuar\")\n",
    "#r #<Response [200]>\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup #translator of html code obtained from requests\n",
    "\n",
    "#soup = BeautifulSoup(r.text, 'html.parser') #makes r.text easier to read in terms of html tags\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find().text #('tag', {id: ''})\n",
    "#soup.find_parent() #replace soup with above variable\n",
    "#soup.find_next_sibling() #replace soup with above variable\n",
    "\n",
    "#soup.text #replace soup with above variable\n",
    "#print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Id : {\n",
    "Id: { ‘product_name’: ‘final fantasy’, ‘description’: ‘game’}\n",
    "ID: ‘1’, {‘product_name’....}\n",
    "So if you’re scraping it and saving it into a data format, you will have to save it as a dictionary with key value pair, the key being the field and the value being the data that is being stored with the key\n",
    "save it as json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os, shutil, os.path\n",
    "\n",
    "image_dict1 = {\"Link\": []}\n",
    "for link in product_links[3:8]:\n",
    "\n",
    "    html_page = requests.get(link).content #send get request for each link in range and get content of each request\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    images = []\n",
    "    for img in soup.find_all(\"img\"): #finds .jpg images only\n",
    "        \n",
    "        images.append(img.get(\"srcset\"))\n",
    "\n",
    "    print(images[2])\n",
    "\n",
    "    try: #download and save product image using product ID as file name\n",
    "         image_link = images[2]\n",
    "         image_link1 = images[2][1]\n",
    "         image_name = f\"test.jpg\"\n",
    "         with open(image_name, \"wb\") as file: #wb = write and binary mode\n",
    "            img = scraper.driver.find_element(By.XPATH, \n",
    "               '//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img')\n",
    "            file.write(img.screenshot_as_png)\n",
    "            #file.write(images[2].jpg)\n",
    "         print(\"Product image downloaded\")\n",
    "         image_dict1[\"Link\"].append(image_link)\n",
    "         #image_dict1[\"Link\"].append(image_link1)\n",
    "         print(\"Image link added\")\n",
    "    except:\n",
    "         image_dict1[\"Link\"].append(\"N/A\")\n",
    "         print(\"Product image not downloaded\")\n",
    "    #print(img)\n",
    "    #check = r\".jpg\"\n",
    "    #for _ in os.listdir(images):\n",
    "     #if _.endswith(check):\n",
    "      #  print(_)\n",
    "#for img in images:\n",
    " #   if img.contains('.jpg'): \n",
    "  #      print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_images(self, container, xpath : str = '//figure[@class=\"product-boxshot-container hover-boxshot\"]'):\n",
    "        '''\n",
    "        This is to find the product images in the search results container \n",
    "        Parameters\n",
    "        -------\n",
    "        xpath: str\n",
    "            locate the images in the results container and their respective links from srcset\n",
    "        '''\n",
    "        list_div = container.find_elements(By.XPATH, '//img[@class=\"product-boxshot lazyloaded\"]')\n",
    "        src_list = []\n",
    "        for product in list_div[:5]:\n",
    "            image_container = product.find_element(By.XPATH, xpath)\n",
    "            src = image_container.find_element(By.XPATH, './/img').get_attribute('srcset')\n",
    "            r = src.split(\"1x\") #split src as srcset contains two links by default\n",
    "            src_list.append(r[0]) #obtain image from 1st link only\n",
    "        return src_list\n",
    "def upload_images(self, src):\n",
    "        '''\n",
    "        This is to save images to a temporary directory and upload them to the s3 bucket \n",
    "        Parameters\n",
    "        -------\n",
    "        src: str\n",
    "            product image links obtained in find_images()\n",
    "        '''\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            for i in range(len(src)):\n",
    "                urllib.request.urlretrieve(src[i], tmpdirname + f'/image_{i}.png')\n",
    "                self.client.upload_file(tmpdirname + f'/image_{i}.png', 'db-1-bucket', f'image_{i}.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96845aca18e994caba59710a135a9f2069490d583ecd78ead3231a9fecf57406"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

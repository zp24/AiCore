{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager #installs Chrome webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    #load webpage in initialiser\n",
    "    def __init__(self, url: str = \"https://store.eu.square-enix-games.com/en_GB/\"): #default url\n",
    "        self.driver = Chrome(ChromeDriverManager().install()) \n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            #driver = Chrome() #specify location of chromedriver if downloading webdriver\n",
    "            print(\"Webpage loaded successfully\")\n",
    "        except:\n",
    "            print(\"Webpage not loaded - please check\")\n",
    "\n",
    "    #click accept cookies button on webpage\n",
    "    def accept_cookies(self, xpath: str = '//*[@id=\"onetrust-accept-btn-handler\"]'): \n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            time.sleep(5)\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "            print(\"'Accept Cookies' button clicked\")\n",
    "        except TimeoutException: #if accept button is not found after 10 seconds by driver\n",
    "            print(\"No cookies found\") \n",
    "\n",
    "    #access and type in search bar\n",
    "    def search_bar(self, text, xpath: str = '//*[@id=\"search-button\"]', \n",
    "                    xpath1: str = '//*[@id=\"search-form-wrapper\"]/form/div/input',\n",
    "                    xpath2: str = '//*[@id=\"search-form-wrapper\"]/form/div/span/button'): \n",
    "        \n",
    "        #click on search bar icon\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "            print(\"Search bar opened\")\n",
    "        except TimeoutException:\n",
    "            print(\"Search bar not found\")\n",
    "        \n",
    "        #open search bar\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, xpath1)))\n",
    "            time.sleep(2)\n",
    "            self.driver.find_element(By.XPATH, xpath1).click()\n",
    "        except TimeoutException:\n",
    "            print(\"Search bar not found - input\")\n",
    "        \n",
    "        #input keywords to search\n",
    "        try:\n",
    "            self.search = self.driver.find_element(By.XPATH, xpath1)\n",
    "            self.search.send_keys(text)\n",
    "            print(\"Search keywords entered\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Cannot input keywords\")\n",
    "        \n",
    "        #submit input\n",
    "        try:\n",
    "            self.search = self.driver.find_element(By.XPATH, xpath2).click()\n",
    "            print(\"Submit search button clicked - redirected to results\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Cannot submit search\")\n",
    "        \n",
    "    \n",
    "    def navigate(self, xpath: str = '//*[@id=\"merchandise\"]'): #navigate tabs - change for games, merchandise or preorders\n",
    "        self.tab_select = self.driver.find_element(By.XPATH,xpath)\n",
    "        time.sleep(2)\n",
    "        self.tab_select.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "    \n",
    "    def find_container(self, xpath: str = '//div[@class=\"catalogue row\"]'):\n",
    "        return self.driver.find_element(By.XPATH, xpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": #will only run methods below if script is run directly\n",
    "    scraper = Scraper() #call scraper class\n",
    "    scraper.accept_cookies()\n",
    "    scraper.navigate()\n",
    "    scraper.search_bar(\"final fantasy\") #add search keyword here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager #installs Chrome webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "container = scraper.find_container()\n",
    "#find many elements that correspond with the XPath - they have to be direct children of the container\n",
    "#i.e. one level below the container\n",
    "list_products = container.find_elements(By.XPATH, './/a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "for product in list_products: #iterate through each product\n",
    "    #print(product.text) #print each product in text format\n",
    "    link_list.append(product.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import uuid #universally unique id\n",
    "\n",
    "#add link and product info to dictionary\n",
    "product_dict = {\"Link\": [], \"Product Name\" :[], \"Price\" : [], \"Description\" : [], \"ID\": [], \"UUID\":[]}\n",
    "image_dict = {\"Link\": [], \"Product UUID\": [], \"Image UUID\": []}\n",
    "\n",
    "def retrieve_data():\n",
    "   #link_list #lists all urls in specified webpage section\n",
    "   for link in link_list[3:8]: #iterates through links 4 to 9 - FFVIII downloads not included (DOB required)\n",
    "      scraper.driver.get(link)\n",
    "      time.sleep(2)\n",
    "      product_dict[\"Link\"].append(link)\n",
    "      try: #get product name\n",
    "         product_name = scraper.driver.find_element(By.XPATH, '//*[@id=\"responsive-wrapper-title\"]/header/h1')\n",
    "            #shortened form of //*[@id=\"responsive-wrapper-title\"]/header/h1\n",
    "            #.//h1[@class=\"product-title\"] - works some of the time\n",
    "         product_dict[\"Product Name\"].append(product_name.text)\n",
    "         print(\"Product name obtained\")\n",
    "      except NoSuchElementException: #not all links accessed will have the same attributes\n",
    "         product_dict[\"Product Name\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get price\n",
    "         price = scraper.driver.find_element(By.XPATH, './/span[@class = \"prices\"]')\n",
    "            #shortened form of //*[@id=\"main-content\"]/article/div[1]/div/div[2]/div[2]/div[1]/div[1]/div/span\n",
    "         product_dict[\"Price\"].append(price.text)\n",
    "         print(\"Price obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict[\"Price\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get product description\n",
    "         desc = scraper.driver.find_element(By.XPATH, './/div[@class=\"tab-pane-content\"]')\n",
    "            #shortened form of //*[@id=\"desc-collapse\"]/div/div\n",
    "         product_dict[\"Description\"].append(desc.text)\n",
    "         print(\"Product description obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict[\"Description\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #get product SKU/ID from URL\n",
    "         #to get SKU - product details tab needs to be clicked and then an if statement to get the SKU otherwise other info will be obtained\n",
    "         r = link.rsplit(\"/\", 6) #split link 6 times after every '/'\n",
    "         product_dict[\"ID\"].append(r[5])\n",
    "\n",
    "         print(\"Product ID obtained\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict[\"ID\"].append(\"N/A\")\n",
    "\n",
    "      time.sleep(3)\n",
    "      try: #generate V4 UUID for product and image\n",
    "         product_uuid = uuid.uuid4()\n",
    "         image_uuid = uuid.uuid4()\n",
    "         product_dict[\"UUID\"].append(product_uuid), image_dict[\"Product UUID\"].append(product_uuid)\n",
    "         image_dict[\"Image UUID\"].append(image_uuid)\n",
    "\n",
    "         print(\"UUID generated\")\n",
    "      except NoSuchElementException:\n",
    "         product_dict[\"UUID\"].append(\"N/A\")\n",
    "         image_dict[\"Product UUID\"].append(\"N/A\")\n",
    "         image_dict[\"Image UUID\"].append(\"N/A\")\n",
    "      \n",
    "      time.sleep(3)\n",
    "      try: #download and save product image using product ID as file name\n",
    "         image_link = f\"{r[5]}.png\"\n",
    "         with open(image_link, \"wb\") as file: #wb = write and binary mode\n",
    "            img = scraper.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/article/div[1]/div/div[1]/div[3]/a/figure/img')\n",
    "            file.write(img.screenshot_as_png)\n",
    "         print(\"Product image downloaded\")\n",
    "         image_dict[\"Link\"].append(image_link)\n",
    "         print(\"Image link added\")\n",
    "      except:\n",
    "         image_dict[\"Link\"].append(\"N/A\")\n",
    "         print(\"Product image not downloaded\")\n",
    "      time.sleep(3)\n",
    "retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dict #display all dictionary entries\n",
    "\n",
    "## print product names and prices ##\n",
    "#print(product_dict[\"Product Name\"], product_dict[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict #display all dictionary entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(product_dict) #displays product dictionary in panda dataframe\n",
    "#pd.DataFrame(image_dict) #displays image dictionary in panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "from uuid import UUID\n",
    "\n",
    "new_folder = \"../webscraping/raw_data\"\n",
    "file = \"data\"\n",
    "create_file = os.path.join(new_folder, file+\".json\") #add file type here\n",
    "\n",
    "#Dealing with no UUID serialization support in json\n",
    "JSONEncoder_olddefault = JSONEncoder.default\n",
    "def JSONEncoder_newdefault(self, o):\n",
    "    if isinstance(o, UUID): return str(o)\n",
    "    return JSONEncoder_olddefault(self, o)\n",
    "JSONEncoder.default = JSONEncoder_newdefault\n",
    "\n",
    "\n",
    "try: #create raw_data folder in current directory - check if folder already exists\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.mkdir(new_folder) #create folder if it doesn't exist\n",
    "        with open(create_file, \"w\") as fp: #specify path here \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "                \n",
    "    elif os.path.exists(new_folder): #if folder already exists\n",
    "        with open(create_file, \"w\") as fp: \n",
    "                json.dump(product_dict, fp,  indent=4)\n",
    "except FileExistsError:\n",
    "    print(\"Already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move files into a new directory/folder\n",
    "import os, shutil, os.path\n",
    "\n",
    "new_folder1 = \"images\" #create images folder in current directory\n",
    "sourcepath = \"../webscraping\"\n",
    "source = os.listdir(sourcepath)\n",
    "destinationpath = \"../webscraping/images\"\n",
    "\n",
    "try:\n",
    " if not os.path.exists(new_folder1):\n",
    "  os.mkdir(new_folder1)\n",
    "  for files in source:\n",
    "      if files.endswith('.png'):\n",
    "          shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files))    \n",
    " elif os.path.exists(new_folder1):\n",
    "      for files in source:\n",
    "        if files.endswith('.png'):\n",
    "            shutil.move(os.path.join(sourcepath, files), os.path.join(destinationpath, files)) \n",
    "except FileExistsError:\n",
    " print(\"Already exists\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96845aca18e994caba59710a135a9f2069490d583ecd78ead3231a9fecf57406"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

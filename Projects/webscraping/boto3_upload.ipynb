{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Could not get version for google-chrome with the command:  powershell \"$ErrorActionPreference='silentlycontinue' ; (Get-Item -Path \"$env:PROGRAMFILES\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion ; if (-not $? -or $? -match $error) { (Get-Item -Path \"$env:PROGRAMFILES(x86)\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion } if (-not $? -or $? -match $error) { (Get-Item -Path \"$env:LOCALAPPDATA\\Google\\Chrome\\Application\\chrome.exe\").VersionInfo.FileVersion } if (-not $? -or $? -match $error) { reg query \"HKCU\\SOFTWARE\\Google\\Chrome\\BLBeacon\" /v version } if (-not $? -or $? -match $error) { reg query \"HKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\Google Chrome\" /v version }\"\n",
      "Current google-chrome version is UNKNOWN\n",
      "Get LATEST chromedriver version for UNKNOWN google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/100.0.4896.60/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\Zoya\\.wdm\\drivers\\chromedriver\\win32\\100.0.4896.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from scraper.webscraper import Scraper\n",
    "\n",
    "scraper = Scraper() #call scraper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper.webscraper import Scraper\n",
    "\n",
    "scraper = Scraper() #call scraper class\n",
    "scraper.accept_cookies()\n",
    "scraper.search_bar('Kingdom Hearts') #add search keyword here\n",
    "container = scraper.find_container()\n",
    "scraper.collect_page_links()\n",
    "src = scraper.find_images(container= container)\n",
    "scraper.upload_images(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add files to bucket \n",
    "import boto3\n",
    "import os\n",
    "import tempfile\n",
    "import sqlalchemy #create an engine which connects local machine/python script with database\n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "client = boto3.client('s3')\n",
    "bucket = os.environ.get('DB_BUCKET')\n",
    "sourcepath = \"images\"\n",
    "source = os.listdir(sourcepath)\n",
    "\n",
    "for file in source:\n",
    "    '''\n",
    "     specify location of files in images directory, otherwise it will try to look \n",
    "     for images in current working directory\n",
    "    '''\n",
    "\n",
    "    with open(f'images/{file}', \"rb\") as f: #ensures each file is looked at individually \n",
    "       client.upload_fileobj(f, bucket, file) #image files will be added to bucket\n",
    "        \n",
    "    #print(file)\n",
    "#source[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlalchemy #create an engine which connects local machine/python script with database\n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "DATABASE_TYPE = os.environ.get('DB_DATABASE_TYPE')\n",
    "DBAPI = os.environ.get('DB_DBAPI') #database API - API to connect Python with database\n",
    "#use AWS details to connect database - saved in Environment Variables\n",
    "HOST = os.environ.get('DB_HOST') #endpoint\n",
    "USER = os.environ.get('DB_USER') #username\n",
    "PASSWORD = os.environ.get('DB_PASS')\n",
    "DATABASE = os.environ.get('DB_DATABASE')\n",
    "PORT = os.environ.get('DB_PORT')\n",
    "\n",
    "engine = create_engine(f'{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x24e7f474eb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload json files to bucket\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('s3')\n",
    "bucket = os.environ.get('DB_BUCKET')\n",
    "'''specify directory where data.json is located\n",
    "    upload_fileobj is used where the file has already been opened\n",
    "    otherwise use upload_file'''\n",
    "#with open(\"raw_data/data.json\", \"rb\") as file: #also works\n",
    " #  response = client.upload_fileobj(file, bucket, \"data.json\") \n",
    "client.upload_file(\"raw_data/data.json\", bucket, \"data.json\")\n",
    "client.upload_file(\"raw_data/image_data.json\", bucket, \"image_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folders and upload files into respective folders in bucket\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('s3')\n",
    "#create folder in s3 bucket\n",
    "bucket = os.environ.get('DB_BUCKET')\n",
    "\n",
    "#folders\n",
    "image_folder = \"images\"\n",
    "data_folder = \"raw_data\"\n",
    "\n",
    "client.put_object(Bucket=bucket, Key=(image_folder+'/'))\n",
    "client.put_object(Bucket=bucket, Key=(data_folder+'/'))\n",
    "\n",
    "#upload files into s3 bucket folders\n",
    "client.upload_file(\"raw_data/data.json\", bucket, f\"{data_folder}/data.json\") #saves data.json file in folder\n",
    "client.upload_file(\"raw_data/image_data.json\", bucket, f\"{data_folder}/image_data.json\")\n",
    "\n",
    "sourcepath = \"images\"\n",
    "source = os.listdir(sourcepath)\n",
    "\n",
    "#upload images to s3 bucket folder\n",
    "for file in source:\n",
    "    with open(f'images/{file}', \"rb\") as f: \n",
    "       '''specify location of files in images directory, otherwise it will try to look \n",
    "            for images in current working directory'''\n",
    "       client.upload_fileobj(f, bucket, f'{image_folder}/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data.json as pandas dataframe and sql table\n",
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "with open('raw_data/data.json') as json_file:\n",
    "    data = json.load(json_file) #load json file\n",
    "    print(\"Type:\", type(data))\n",
    "    df_products = pd.DataFrame(data) #save data in json file in dataframe\n",
    "sql_table = df_products.to_sql(\"SE_products\", con=scraper.engine, if_exists='replace', index=False) #upload pandas dataframe to sql table \n",
    "                                                                                                   #use replace instead of append if changing everything in sql table\n",
    "sql_table #prints number of entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save image_data.json as pandas dataframe and sql table\n",
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "with open('raw_data/image_data.json') as json_file:\n",
    "    data = json.load(json_file) #load json file\n",
    "    print(\"Type:\", type(data))\n",
    "    df_images = pd.DataFrame(data) #save data in json file in dataframe\n",
    "sql_table_i = df_images.to_sql(\"SE_product_images\", con=scraper.engine, if_exists='replace', index=False) #upload pandas dataframe to sql table \n",
    "                                                                                                   #use replace instead of append if changing everything in sql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge tables on product_uuid column\n",
    "import pandas as pd\n",
    "\n",
    "se_products = pd.merge(df_products, df_images, on= 'product_uuid', how=\"outer\")\n",
    "se_table = se_products.to_sql(\"SE_products_final\", con=scraper.engine, if_exists='replace', index=False) #upload pandas dataframe to sql table \n",
    "\n",
    "se_products"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96845aca18e994caba59710a135a9f2069490d583ecd78ead3231a9fecf57406"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
